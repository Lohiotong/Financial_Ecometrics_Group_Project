import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler

# === Step 1: åŠ è½½æ¸…æ´—æ•°æ® ===
df = pd.read_csv("data/interim/cleaned/cleaned_all_companies_long.csv")
df['Dates'] = pd.to_datetime(df['Dates'], errors='coerce')
df = df.sort_values(['Company', 'Dates'])

# === Step 2: æ„é€ è¡ç”Ÿç‰¹å¾ï¼ˆåŸºäºå†å²æ•°æ®ï¼Œæ— æœªæ¥æ³„æ¼ï¼‰===
df['return_lag1'] = df.groupby('Company')['PX_LAST'].pct_change(1)
df['momentum_5d'] = df.groupby('Company')['PX_LAST'].pct_change(5)
df['momentum_10d'] = df.groupby('Company')['PX_LAST'].pct_change(10)
df['volatility_5d'] = df.groupby('Company')['PX_LAST'].rolling(5).std().reset_index(level=0, drop=True)
df['volatility_10d'] = df.groupby('Company')['PX_LAST'].rolling(10).std().reset_index(level=0, drop=True)
df['PE_log'] = np.log1p(df['PE_RATIO'])
df['VIX_PE_interact'] = df['VIX Index'] * df['PE_RATIO']
df['USDJPY_ret'] = df['USDJPY Curncy'].pct_change(1)

# === Step 3: è®¾å®šè®­ç»ƒå…¬å¸ï¼ˆç”¨äºé€‰å› å­ + æ¨¡å‹è®­ç»ƒï¼‰===
train_companies = [
    "ADBE", "AMD", "AMZN", "AVGO", "CSCO", "GOOGL", "IBM",
    "INTC", "META", "MSFT", "NVDA", "CRM"
]  # ğŸ§  AAPL, ORCL, TXN ä¿ç•™ç”¨äºæ³›åŒ–éªŒè¯

# === Step 4: ç­›é€‰è®­ç»ƒæ•°æ®ï¼ˆå…¬å¸ + å¹´ä»½ 2020â€“2022ï¼‰===
train_df = df[
    (df['Company'].isin(train_companies)) &
    (df['Dates'].dt.year <= 2022)
].copy()

# === Step 5: æ„é€ ç›®æ ‡å˜é‡ï¼ˆ10æ—¥å¯¹æ•°æ”¶ç›Šæ»‘åŠ¨å¹³å‡ï¼‰===
train_df['log_return'] = train_df.groupby('Company')['PX_LAST'].transform(lambda x: np.log(x).diff())
train_df['return'] = (
    train_df.groupby('Company')['log_return']
    .shift(-1)
    .rolling(10)
    .mean()
    .shift(-9)
    .clip(-0.3, 0.3)
)
train_df = train_df.dropna(subset=['return'])

# === Step 6: ç‰¹å¾åˆ—ï¼ˆæ’é™¤å…ƒæ•°æ®åˆ—ï¼‰===
exclude_cols = ['Dates', 'Company', 'PX_LAST', 'PX_OPEN', 'PX_HIGH', 'PX_LOW',
                'PX_VOLUME.1', 'log_return', 'return']
features = [col for col in train_df.columns if col not in exclude_cols]

# === Step 7: ç‰¹å¾æ ‡å‡†åŒ– + éšæœºæ£®æ—é€‰å› å­ ===
X = train_df[features].fillna(0)
y = train_df['return'].fillna(0)
X_scaled = StandardScaler().fit_transform(X)

rf = RandomForestRegressor(n_estimators=200, max_depth=6, random_state=42)
rf.fit(X_scaled, y)

# === Step 8: æå–å¹¶ä¿å­˜ Top 20 å› å­ ===
importances = pd.Series(rf.feature_importances_, index=features).sort_values(ascending=False)
top_20_factors = importances.head(20)

print("ğŸ“Œ è¡Œä¸šç»Ÿä¸€é€‰å‡ºçš„ Top20 å› å­ï¼š")
print(top_20_factors)

top_20_factors.to_frame(name='importance').to_csv("data/result/selected_factors.csv", index=True)
print("å·²ä¿å­˜ Top 20 å› å­åˆ° data/result/selected_factors.csv")
